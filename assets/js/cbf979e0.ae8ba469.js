"use strict";(globalThis.webpackChunkai_spec_book=globalThis.webpackChunkai_spec_book||[]).push([[1964],{6130:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"the-spec-driven-ai-engineer/chapter-3-openai-stack","title":"Chapter 3: The OpenAI Agentic Stack","description":"The release of GPT-4 marked a turning point, but the true revolution in AI development is the ecosystem being built around the models. The paradigm is shifting from simple, one-shot \\"prompt-and-response\\" to persistent, tool-using agents. OpenAI provides a powerful, if sometimes opaque, stack for building these agents.","source":"@site/docs/the-spec-driven-ai-engineer/03-chapter-3-openai-stack.md","sourceDirName":"the-spec-driven-ai-engineer","slug":"/spec-driven-ai-engineer/openai-stack","permalink":"/ai-spec-book/spec-driven-ai-engineer/openai-stack","draft":false,"unlisted":false,"editUrl":"https://github.com/waterprooffish99/ai-spec-book/tree/main/docs/the-spec-driven-ai-engineer/03-chapter-3-openai-stack.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"slug":"/spec-driven-ai-engineer/openai-stack","title":"Chapter 3: The OpenAI Agentic Stack"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Building Blocks - RAG and Vector Databases","permalink":"/ai-spec-book/spec-driven-ai-engineer/rag-and-vector-db"},"next":{"title":"Chapter 4: Advanced Architectures - Subagents and Delegation","permalink":"/ai-spec-book/spec-driven-ai-engineer/advanced-architectures"}}');var i=t(4848),a=t(8453);const r={slug:"/spec-driven-ai-engineer/openai-stack",title:"Chapter 3: The OpenAI Agentic Stack"},o=void 0,l={},d=[{value:"From Completions to Assistants",id:"from-completions-to-assistants",level:2},{value:"The Core Objects",id:"the-core-objects",level:3},{value:"Building a Simple RAG Agent with Assistants API",id:"building-a-simple-rag-agent-with-assistants-api",level:2},{value:"What&#39;s Happening Under the Hood?",id:"whats-happening-under-the-hood",level:3}];function c(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:['The release of GPT-4 marked a turning point, but the true revolution in AI development is the ecosystem being built around the models. The paradigm is shifting from simple, one-shot "prompt-and-response" to persistent, tool-using ',(0,i.jsx)(n.strong,{children:"agents"}),". OpenAI provides a powerful, if sometimes opaque, stack for building these agents."]}),"\n",(0,i.jsxs)(n.p,{children:["This chapter will focus on the conceptual framework of the ",(0,i.jsx)(n.strong,{children:"Assistants API"})," and how to interact with it. While the hosted OpenAI platform is excellent for rapid prototyping, understanding these principles is key to building custom, open-source agentic systems later."]}),"\n",(0,i.jsx)(n.h2,{id:"from-completions-to-assistants",children:"From Completions to Assistants"}),"\n",(0,i.jsxs)(n.p,{children:["The original way to interact with an LLM was through the ",(0,i.jsx)(n.strong,{children:"Completions API"}),". You sent a prompt, and the model returned a completion. This is a stateless, one-time transaction."]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"Assistants API"})," introduces the concept of state and capabilities. An ",(0,i.jsx)(n.code,{children:"Assistant"})," is a more persistent entity that you configure with:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Instructions:"}),' A detailed prompt defining its persona and objective (the "Spec").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model:"})," The underlying LLM to use (e.g., ",(0,i.jsx)(n.code,{children:"gpt-4-turbo-preview"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tools:"})," A list of functions the Assistant can decide to call to get external information or perform actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Knowledge:"})," A set of files that OpenAI will automatically manage in a RAG system for you."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"the-core-objects",children:"The Core Objects"}),"\n",(0,i.jsx)(n.p,{children:"Interacting with the Assistants API involves a few key objects:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Assistant:"})," The configuration of your agent. You create this once."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Thread:"})," A conversation session. A thread contains all the messages in a conversation. You create one thread per user or per conversation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Message:"})," A single turn in the conversation, from either the ",(0,i.jsx)(n.code,{children:"user"})," or the ",(0,i.jsx)(n.code,{children:"assistant"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Run:"})," An execution of the Assistant on a Thread. When you add a new user message, you create a ",(0,i.jsx)(n.code,{children:"Run"}),". The Assistant then takes over, reading the thread, deciding whether to call tools or generate a response, and adding its own message to the thread."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:['This stateful, asynchronous model is what makes it "agentic." The Assistant can decide on a multi-step reasoning process, call multiple tools, and even correct itself within a single ',(0,i.jsx)(n.code,{children:"Run"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"building-a-simple-rag-agent-with-assistants-api",children:"Building a Simple RAG Agent with Assistants API"}),"\n",(0,i.jsx)(n.p,{children:"Let's build an agent that can answer questions about the documents we indexed in the previous chapter. For this example, instead of using our own Qdrant instance, we'll upload a file directly to OpenAI and let it handle the RAG implementation."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Install Libraries:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"2. Set up your environment:"}),"\nMake sure you have your OpenAI API key set as an environment variable."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"export OPENAI_API_KEY='sk-...'\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["3. Create a knowledge file (",(0,i.jsx)(n.code,{children:"knowledge.txt"}),"):"]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"The first AI agent was named Shakey and was developed at Stanford in 1966.\nSpec-Driven Development provides a framework for building reliable agentic systems.\nQdrant is a high-performance vector database built in Rust.\nUnlike Shakey, modern AI agents leverage the power of large language models.\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["4. Create the Python script (",(0,i.jsx)(n.code,{children:"assistant_example.py"}),"):"]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import time\nimport os\nfrom openai import OpenAI\n\n# Initialize the client\nclient = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))\n\n# --- 1. Create Knowledge File and Assistant ---\n\n# Upload our knowledge file to OpenAI\nfile = client.files.create(\n  file=open("knowledge.txt", "rb"),\n  purpose=\'assistants\'\n)\nprint(f"\ud83d\udcc4 Uploaded knowledge file: {file.id}")\n\n# Create the Assistant\n# We enable the "Retrieval" tool (OpenAI\'s managed RAG)\n# And we associate our uploaded file with it.\nassistant = client.beta.assistants.create(\n  name="Spec-Driven AI Expert",\n  instructions="You are an expert on AI development. Use your knowledge base to answer questions about agentic systems and AI history.",\n  model="gpt-4-turbo-preview",\n  tools=[{"type": "retrieval"}],\n  file_ids=[file.id]\n)\nprint(f"\ud83e\udd16 Created Assistant: {assistant.id}")\n\n\n# --- 2. Create a Thread and Run the Conversation ---\n\n# Create a conversation thread\nthread = client.beta.threads.create()\nprint(f"\ud83e\uddf5 Created Thread: {thread.id}")\n\n# Add the user\'s first message to the thread\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role="user",\n    content="What\'s the difference between early AI agents and modern ones?"\n)\n\n# Create a "Run" to process the thread\nrun = client.beta.threads.runs.create(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n)\nprint(f"\ud83c\udfc3\u200d\u2642\ufe0f Created Run: {run.id}")\n\n\n# --- 3. Wait for Completion and Display the Result ---\n\n# The Run is asynchronous. We need to poll until it\'s done.\nwhile run.status in [\'queued\', \'in_progress\', \'cancelling\']:\n  time.sleep(1)\n  run = client.beta.threads.runs.retrieve(\n    thread_id=thread.id,\n    run_id=run.id\n  )\n  print(f"  - Run status: {run.status}")\n\n\nif run.status == \'completed\': \n  # List the messages in the thread\n  messages = client.beta.threads.messages.list(\n    thread_id=thread.id\n  )\n\n  # Find the assistant\'s response\n  # Messages are returned in descending order\n  assistant_message = next(m for m in messages.data if m.role == \'assistant\')\n  \n  print("\\n\u2705 Assistant Response:")\n  # The content is an array, get the first text element\n  response_text = assistant_message.content[0].text.value\n  print(response_text)\n\n  # OpenAI\'s Retrieval tool often adds citations\n  annotations = assistant_message.content[0].text.annotations\n  if annotations:\n    print("\\n\ud83d\udcda Citations:")\n    print(annotations[0].file_citation)\n\nelse:\n  print(f"\\n\u274c Run failed with status: {run.status}")\n  print(run.last_error)\n\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"5. Run the script:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python assistant_example.py\n"})}),"\n",(0,i.jsx)(n.h3,{id:"whats-happening-under-the-hood",children:"What's Happening Under the Hood?"}),"\n",(0,i.jsxs)(n.p,{children:["When you create the ",(0,i.jsx)(n.code,{children:"Run"}),", the Assistant executes its core loop:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Analyze:"})," It reads the entire thread history."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Decide:"})," It sees the user's question. Because it has the ",(0,i.jsx)(n.code,{children:"retrieval"})," tool enabled, it decides it needs to search its knowledge base."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Act:"}),' It formulates a search query (e.g., "early AI agents vs modern AI agents") and runs it against the ',(0,i.jsx)(n.code,{children:"knowledge.txt"})," file you provided."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Observe:"}),' It gets the search results, which include the lines about "Shakey" and "modern AI agents leverage LLMs."']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generate:"})," It synthesizes these retrieved facts into a coherent, natural language response, citing the source."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Respond:"})," It adds this new message to the thread."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["This loop of ",(0,i.jsx)(n.code,{children:"Analyze -> Decide -> Act -> Observe -> Generate"})," is the fundamental pattern of all agentic systems. While the Assistants API abstracts much of this away, mastering this pattern is essential for building advanced, custom architectures, as we'll see in the next chapter. The hosted solution is a great starting point, but true power comes from controlling this loop yourself."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);
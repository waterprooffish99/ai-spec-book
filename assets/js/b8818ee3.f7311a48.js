"use strict";(globalThis.webpackChunkai_spec_book=globalThis.webpackChunkai_spec_book||[]).push([[9801],{7634:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>L,contentTitle:()=>I,default:()=>R,frontMatter:()=>S,metadata:()=>r,toc:()=>C});const r=JSON.parse('{"id":"the-spec-driven-ai-engineer/chapter-2-rag-and-vector-db","title":"Chapter 2: Building Blocks - RAG and Vector Databases","description":"The Knowledge Problem","source":"@site/docs/the-spec-driven-ai-engineer/02-chapter-2-rag-and-vector-db.md","sourceDirName":"the-spec-driven-ai-engineer","slug":"/spec-driven-ai-engineer/rag-and-vector-db","permalink":"/ai-spec-book/spec-driven-ai-engineer/rag-and-vector-db","draft":false,"unlisted":false,"editUrl":"https://github.com/waterprooffish99/ai-spec-book/tree/main/docs/the-spec-driven-ai-engineer/02-chapter-2-rag-and-vector-db.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"slug":"/spec-driven-ai-engineer/rag-and-vector-db","title":"Chapter 2: Building Blocks - RAG and Vector Databases"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: The Philosophy of Spec-Driven Development","permalink":"/ai-spec-book/spec-driven-ai-engineer/philosophy"},"next":{"title":"Chapter 3: The OpenAI Agentic Stack","permalink":"/ai-spec-book/spec-driven-ai-engineer/openai-stack"}}');var a=t(4848),s=t(8453),o=t(6540),i=t(4164),l=t(7559),c=t(3104),d=t(6347),u=t(205),h=t(7485),p=t(1682),m=t(679);function g(e){return o.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function f(e){const{values:n,children:t}=e;return(0,o.useMemo)(()=>{const e=n??function(e){return g(e).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r}))}(t);return function(e){const n=(0,p.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function x({value:e,tabValues:n}){return n.some(n=>n.value===e)}function b({queryString:e=!1,groupId:n}){const t=(0,d.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,h.aZ)(r),(0,o.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}function v(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,a=f(e),[s,i]=(0,o.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!x({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a})),[l,c]=b({queryString:t,groupId:r}),[d,h]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,r]=(0,m.Dv)(n);return[t,(0,o.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),p=(()=>{const e=l??d;return x({value:e,tabValues:a})?e:null})();(0,u.A)(()=>{p&&i(p)},[p]);return{selectedValue:s,selectValue:(0,o.useCallback)(e=>{if(!x({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);i(e),c(e),h(e)},[c,h,a]),tabValues:a}}var j=t(2303);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function w({className:e,block:n,selectedValue:t,selectValue:r,tabValues:s}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,c.a_)(),d=e=>{const n=e.currentTarget,a=o.indexOf(n),i=s[a].value;i!==t&&(l(n),r(i))},u=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,a.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:r})=>(0,a.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:u,onClick:d,...r,className:(0,i.A)("tabs__item",y.tabItem,r?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function k({lazy:e,children:n,selectedValue:t}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===t);return e?(0,o.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,a.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function T(e){const n=v(e);return(0,a.jsxs)("div",{className:(0,i.A)(l.G.tabs.container,"tabs-container",y.tabList),children:[(0,a.jsx)(w,{...n,...e}),(0,a.jsx)(k,{...n,...e})]})}function _(e){const n=(0,j.A)();return(0,a.jsx)(T,{...e,children:g(e.children)},String(n))}const q={tabItem:"tabItem_Ymn6"};function A({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,i.A)(q.tabItem,t),hidden:n,children:e})}const S={slug:"/spec-driven-ai-engineer/rag-and-vector-db",title:"Chapter 2: Building Blocks - RAG and Vector Databases"},I=void 0,L={},C=[{value:"The Knowledge Problem",id:"the-knowledge-problem",level:2},{value:"What is RAG?",id:"what-is-rag",level:2},{value:"Vector Databases: The Engine of RAG",id:"vector-databases-the-engine-of-rag",level:2},{value:"How it Works: Embeddings",id:"how-it-works-embeddings",level:3},{value:"Introducing Qdrant",id:"introducing-qdrant",level:3},{value:"Practical Example: Setting up and Using Qdrant",id:"practical-example-setting-up-and-using-qdrant",level:3}];function D(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"the-knowledge-problem",children:"The Knowledge Problem"}),"\n",(0,a.jsx)(n.p,{children:"Large Language Models (LLMs) are trained on vast, but static, snapshots of the internet. This leads to two fundamental problems:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"The Knowledge Cutoff:"})," The model has no information about events, data, or documentation created after its training date."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"The Hallucination Problem:"}),' When asked about topics outside its training data (or even on its fringes), an LLM will often "hallucinate"\u2014generating plausible but factually incorrect information.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"The Privacy Problem:"})," You cannot use an LLM to answer questions about your private, proprietary data (e.g., internal company documents, user data) because the model has never seen it."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The solution to all three is ",(0,a.jsx)(n.strong,{children:"Retrieval-Augmented Generation (RAG)"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"what-is-rag",children:"What is RAG?"}),"\n",(0,a.jsxs)(n.p,{children:["RAG is a design pattern that gives an LLM access to external, real-time information. Instead of relying solely on its internal, trained knowledge, the agent first ",(0,a.jsx)(n.strong,{children:"retrieves"})," relevant data from an outside source and then uses that data to ",(0,a.jsx)(n.strong,{children:"augment"})," its prompt when ",(0,a.jsx)(n.strong,{children:"generating"})," a response."]}),"\n",(0,a.jsx)(n.p,{children:"The flow is simple but powerful:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"User Query:"}),' The user asks a question, e.g., "What were our Q3 sales figures?"']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Retrieval:"}),' The system doesn\'t immediately ask the LLM. First, it searches an external knowledge base (like a database of company sales reports) for documents relevant to "Q3 sales figures."']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Augmentation:"})," The system takes the most relevant documents it found and prepends them to the user's query as context."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Generation:"})," The system sends this augmented prompt to the LLM. The final prompt looks something like this:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'Context:\n"Document 7: Q3 Sales Report. Total revenue was $4.2M, with a 12% increase in the enterprise sector..."\n"Document 12: Q3 Regional Breakdown. North America contributed $2.8M..."\n\n---\n\nBased on the context provided, answer the following question: What were our Q3 sales figures?\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Response:"})," The LLM, now equipped with the necessary facts, can generate a correct and fact-based answer."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"vector-databases-the-engine-of-rag",children:"Vector Databases: The Engine of RAG"}),"\n",(0,a.jsxs)(n.p,{children:['The magic of the "Retrieval" step lies in ',(0,a.jsx)(n.strong,{children:"vector databases"}),'. To find "relevant" documents, we need to search by semantic meaning, not just keywords.']}),"\n",(0,a.jsx)(n.h3,{id:"how-it-works-embeddings",children:"How it Works: Embeddings"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Indexing:"}),' When you add a document to your knowledge base, you first use a special kind of model (an "embedding model") to convert the text into a high-dimensional vector\u2014an array of numbers like ',(0,a.jsx)(n.code,{children:"[0.02, -0.5, 0.9, ...]"}),'.\nThis vector represents the document\'s semantic meaning. Documents with similar meanings will have vectors that are "close" to each other in vector space.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Querying:"})," When a user asks a question, you convert the ",(0,a.jsx)(n.em,{children:"query itself"})," into a vector using the same embedding model."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Similarity Search:"}),' You then ask the vector database to find the document vectors that are closest to your query vector. The most common way to measure "closeness" is ',(0,a.jsx)(n.strong,{children:"Cosine Similarity"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:'This process allows us to find documents that are about "quarterly earnings" even if the user asks about "how much money we made."'}),"\n",(0,a.jsx)(n.h3,{id:"introducing-qdrant",children:"Introducing Qdrant"}),"\n",(0,a.jsxs)(n.p,{children:["While there are many vector databases, ",(0,a.jsx)(n.strong,{children:"Qdrant"})," is a powerful, open-source, and production-ready option. It's built in Rust for performance and offers a rich set of features."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Core Qdrant Concepts:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Collection:"})," A named set of points, analogous to a table in a SQL database. You would typically have one collection per type of document (e.g., ",(0,a.jsx)(n.code,{children:"internal_docs"}),", ",(0,a.jsx)(n.code,{children:"support_tickets"}),")."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Point:"})," The fundamental unit in Qdrant. Each point has:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["A unique ",(0,a.jsx)(n.strong,{children:"ID"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["A ",(0,a.jsx)(n.strong,{children:"Vector"})," (the embedding)."]}),"\n",(0,a.jsxs)(n.li,{children:["An optional ",(0,a.jsx)(n.strong,{children:"Payload"})," (a JSON object containing the original text content or other metadata, like ",(0,a.jsx)(n.code,{children:'source: "doc_123.pdf"'}),")."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"practical-example-setting-up-and-using-qdrant",children:"Practical Example: Setting up and Using Qdrant"}),"\n",(0,a.jsx)(n.p,{children:"Let's build a simple RAG system."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"1. Install Libraries:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install qdrant-client sentence-transformers\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"2. Run Qdrant with Docker:"}),"\nThe easiest way to get started is with Docker."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker run -p 6333:6333 qdrant/qdrant\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This will start a Qdrant instance on ",(0,a.jsx)(n.code,{children:"localhost:6333"}),"."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.strong,{children:["3. Create a Python script (",(0,a.jsx)(n.code,{children:"rag_example.py"}),"):"]})}),"\n",(0,a.jsx)(_,{children:(0,a.jsx)(A,{value:"python",label:"rag_example.py",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from qdrant_client import QdrantClient, models\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize the embedding model\n# This model will run locally on your machine\nencoder = SentenceTransformer(\'all-MiniLM-L6-v2\') \n\n# Initialize the Qdrant client\nqdrant = QdrantClient("localhost", port=6333)\n\n# --- 1. Indexing Phase ---\n\n# Create a new collection in Qdrant\n# We tell it the size of the vectors we\'ll be using (384 for \'all-MiniLM-L6-v2\')\n# And the distance metric to use (Cosine similarity)\nqdrant.recreate_collection(\n    collection_name="my_documents",\n    vectors_config=models.VectorParams(size=encoder.get_sentence_embedding_dimension(), distance=models.Distance.COSINE)\n)\n\n# Let\'s create some documents to index\ndocuments = [\n    {"id": 1, "text": "The first AI agent was named Shakey and was developed at Stanford in 1966.", "source": "history_of_ai.pdf"},\n    {"id": 2, "text": "Spec-Driven Development provides a framework for building reliable agentic systems.", "source": "sdd_whitepaper.doc"},\n    {"id": 3, "text": "Qdrant is a high-performance vector database built in Rust.", "source": "qdrant_docs.html"},\n    {"id": 4, "text": "Unlike Shakey, modern AI agents leverage the power of large language models.", "source": "modern_robotics.pdf"},\n]\n\n# Add the documents to our collection\nqdrant.upload_records(\n    collection_name="my_documents",\n    records=[\n        models.Record(\n            id=doc["id"],\n            vector=encoder.encode(doc["text"]).tolist(),\n            payload={"text": doc["text"], "source": doc["source"]}\n        ) for doc in documents\n    ]\n)\n\nprint("\u2705 Documents indexed.")\n\n\n# --- 2. Retrieval Phase ---\n\nuser_query = "What are modern AI agents?"\n\n# Convert the query to a vector\nquery_vector = encoder.encode(user_query).tolist()\n\n# Perform the similarity search in Qdrant\nhits = qdrant.search(\n    collection_name="my_documents",\n    query_vector=query_vector,\n    limit=2 # Find the top 2 most similar documents\n)\n\nprint("\\n\ud83d\udd0d Found relevant documents:")\nfor hit in hits:\n    print(f"  - Source: {hit.payload[\'source\']}, Score: {hit.score:.4f}")\n    print(f"    Text: {hit.payload[\'text\']}")\n\n# In a real application, you would now take the text from these hits,\n# format it into a prompt, and send it to an LLM.\n'})})})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"4. Run the script:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python rag_example.py\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\u2705 Documents indexed.\n\n\ud83d\udd0d Found relevant documents:\n  - Source: modern_robotics.pdf, Score: 0.8123\n    Text: Unlike Shakey, modern AI agents leverage the power of large language models.\n  - Source: sdd_whitepaper.doc, Score: 0.6543\n    Text: Spec-Driven Development provides a framework for building reliable agentic systems.\n"})}),"\n",(0,a.jsx)(n.p,{children:'As you can see, the system correctly identified the most relevant document about "modern AI agents" and also found a tangentially related document about agentic systems. This retrieved context is the raw material you would feed to your LLM to ensure a grounded, factual answer, forming the backbone of any serious agentic workflow.'})]})}function R(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(D,{...e})}):D(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>i});var r=t(6540);const a={},s=r.createContext(a);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);